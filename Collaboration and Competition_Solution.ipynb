{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from ddpg_agent import Agent\n",
    "import random\n",
    "import torch\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis.app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_name = env.brain_names[0] # get the default brain\n",
    "brain = env.brains[brain_name]\n",
    "num_agents = 2\n",
    "action_size = brain.vector_action_space_size\n",
    "env_info = env.reset(train_mode=True)[brain_name]     # reset the environment \n",
    "agent = Agent(state_size=24, action_size=action_size, random_seed=11)        \n",
    "\n",
    "def ddpg(n_episodes=1000, max_t=800, print_every=100):   \n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    average_deque = []    \n",
    "    for i_episode in range(1, n_episodes+1):      \n",
    "        env_info = env.reset(train_mode = True)[brain_name]  # reset the environment  \n",
    "        agent.reset()\n",
    "        scores_a = np.zeros(num_agents)                      # initialize the score (for each agent)\n",
    "        states = env_info.vector_observations                # get the current state (for each agent)\n",
    "        for t in range(max_t):     \n",
    "            actions = agent.act(state = states, add_noise = True)         # select an action (for each agent) ,changed add noise to False\n",
    "            actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)           \n",
    "            rewards = env_info.rewards                         # get reward (for each agent)   \n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            scores_a += env_info.rewards                         # update the score (for each agent)\n",
    "            for k in range(0,num_agents):  \n",
    "                agent.step(state = states[k], action = actions[k], \n",
    "                           reward = rewards[k], next_state = next_states[k], \n",
    "                           done = dones[k])\n",
    "            states = next_states                              # roll over states to next time step\n",
    "            if np.any(dones):                                  # exit loop if episode finished\n",
    "                break\n",
    "\n",
    "        scores.append(np.max(scores_a))\n",
    "        scores_deque.append(scores)  \n",
    "        average_deque.append(np.mean(scores_deque))\n",
    "        print('\\rScore (max over agents) from episode {}: {},{}'.format(i_episode, np.max(scores_a),\n",
    "                                                                        np.mean(scores_deque)), end=\"\")\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rScore (Average Score) from episode {}: {}'.format(i_episode, np.mean(scores_deque)))\n",
    "         \n",
    "        if i_episode % print_every == 0 and np.mean(scores_deque)>0.52:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            break\n",
    "            \n",
    "    return scores,average_deque\n",
    "\n",
    "scores,average_deque = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores,average_deque)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
